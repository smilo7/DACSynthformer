{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from utils.utils import generate_mask, load_model, writeDACFile, sample_top_n\n",
    "from dataloader.dataset import CustomDACDataset\n",
    "from utils.utils import interpolate_vectors, breakpoints, breakpoints_classseq\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from DACTransformer.RopeCondDACTransformer import RopeCondDACTransformer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dac\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087dd2f6-8fee-47fa-86e3-0db3cddef909",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be7109-af19-4766-b0c8-4f800223ded0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3286900-9141-49a6-bcb2-59b37836e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## params ##########################################################\n",
    "# set this to whatever you called the experiment in the top of your params.yaml file.\n",
    "experiment_name= \"mini_test_01\" #\"smalltest_dataset\" \n",
    "# probably don't change this is the default, set in the params.yaml file.\n",
    "checkpoint_dir = 'runs' + '/' + experiment_name  \n",
    "\n",
    "cptnum =  10 # (the checkpoint number must be in the checkpoint directory)\n",
    "SAVEWAV=False\n",
    "DEVICE='cpu' #######''cuda'\n",
    "gendur=20 #how many seconds you wnat your output sound to be\n",
    "topn=20 # sample from the top n logits\n",
    "\n",
    "###########################################################################\n",
    "#  Choose a breakpoint sequence (and/or make one yourself) ...\n",
    "###########################################################################\n",
    "morphname='conditioning'  ###   (choose from breakpoint sets defined below)\n",
    "#morphname='sweep'  ###   (choose from breakpoint sets defined below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c7b47-c281-4e0e-a61b-7eec0490f883",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808153c-d60e-4ce5-9146-f46e36e1b4c8",
   "metadata": {},
   "source": [
    "### Read Paramfile and get class list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5cda0-aa32-47e5-a4cd-1f3f1bd3cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#any config.yaml files used for training are copied to the checkpoint directory as \"params.yaml\"\n",
    "paramfile = checkpoint_dir + '/' +  'params.yaml' \n",
    "print(f\"will use paramfile= {paramfile}\") \n",
    "# Load YAML file\n",
    "with open(paramfile, 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "# Create an instance of the dataset\n",
    "data_dir = params['data_dir']\n",
    "data_frames =  params['data_frames']\n",
    "dataset = CustomDACDataset(data_dir=data_dir, metadata_excel=data_frames, transforms=None)\n",
    "\n",
    "#For your reference:\n",
    "#Print the list of all classes\n",
    "classes=dataset.get_class_list()\n",
    "print(f'classes={classes}')\n",
    "print(f' ------- One hot vectors for classes ----------')\n",
    "for i in range(len(classes)):\n",
    "    print(f' {classes[i]} : \\t{dataset.onehot(classes[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1b421-0983-459e-911e-37c8a0ecc603",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#  These first four just explore the parameter range for each class used for training ...\n",
    "###########################################################################\n",
    "morphs={}\n",
    "\n",
    "# Create a 'breakpoint' param sweep from 0 upt to 1 and back for a class (just change the index for different classes)\n",
    "#---------------------------------------------------------------------------------\n",
    "morphs['sweep']={\n",
    "    'vsequence' : breakpoints(classes+[\"param\"], \n",
    "                                     **{classes[0] : [1,1,1,1,1,1]}, # a keyword dictionary to use a string as a key!\n",
    "                                     # DSApplause = [1,1,1,1,1,1],   # this works, too, if you prefer\n",
    "                                     param=[0,0,1,1,0,0]),\n",
    "    'vtimes' : [0,.1, .4, .6, .9, 1]\n",
    "}\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Create a sequence 'breakpoints' that steps through each class in the list\n",
    "morphs['conditioning']=breakpoints_classseq(classes, [.5])\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Morph over a vectors in vsequence lineary for (noramlized) time steps vtimes. Create your sequence explicitly\"\n",
    "# It might be easier to use breakpoints() as above to create this morph\n",
    "# Don't specify classes that you don't have on the list!\n",
    "if 0: \n",
    "    morphs['0.1.overlap'] = {\n",
    "        'vsequence' : breakpoints(classes+[\"param\"],\n",
    "                                  **{classes[0] : [1,1,1,1,0,0]},\n",
    "                                  **{classes[2] : [0,0,1,1,1,1]},\n",
    "                                  param=[.5,.5,.5,.5,.5,.5]),\n",
    "        'vtimes' : [0,.2,.4,.6,.8, 1] # must be the same length as the number of break points in vsequence\n",
    "    }\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# Create your sequence explicitly\"\n",
    "# Just set class and param values at half-mast and run for the whole length of time \n",
    "# Vectors must be the proper length - num classes + num parameters!!!!!! \n",
    "# note: each column corresponds to a class value. Rows correspond to vtimes. \n",
    "if 0: \n",
    "    morphs['allmid'] = {\n",
    "        'vsequence': [\n",
    "            torch.tensor([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n",
    "            torch.tensor([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "        ],\n",
    "        'vtimes': [0,1] # must be the same length as the number of break points in vsequence\n",
    "    }\n",
    "\n",
    "\n",
    "#====================================================================================\n",
    "#print out your chosen morph breakpoints\n",
    "morphs[morphname]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05903b1",
   "metadata": {},
   "source": [
    "Morph over a vectors in vsequence lineary for (noramlized) time steps vtimes. Create your sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fa6fb-0f5f-4f02-82bf-46394645da80",
   "metadata": {},
   "source": [
    "### <font color='blue'> Derived parameters  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters from yaml file and derive any necessary\n",
    "######################################################\n",
    "\n",
    "inference_steps=86*gendur  #86 frames per second\n",
    "    \n",
    "TransformerClass =  globals().get(params['TransformerClass'])  \n",
    "print(f\"using TransformerClass = {params['TransformerClass']}\") \n",
    "print(f' and TransformerClass is class object {TransformerClass}')\n",
    "\n",
    "cond_size = 8 # num_classes + num params - not a FREE parameter!\n",
    "\n",
    "### embed_size = params['tblock_input_size'] -cond_size # 240 #32  # embed_size must be divisible by num_heads and by num tokens\n",
    "embed_size = params['model_size'] # 240 #32  # embed_size must be divisible by num_heads and by num tokens\n",
    "print(f'embed_size is {embed_size}')\n",
    "\n",
    "\n",
    "fnamebase='out' + '.e' + str(embed_size) + '.l' + str(params['num_layers']) + '.h' + str(params['num_heads']) + '_chkpt_' + str(cptnum).zfill(4) \n",
    "checkpoint_path = checkpoint_dir + '/' +  fnamebase  + '.pth' \n",
    "\n",
    "# for saving sound \n",
    "outdir=checkpoint_dir\n",
    "\n",
    "print(f'checkpoint_path = {checkpoint_path}, fnamebase = {fnamebase}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59747826",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE == 'cuda' :\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "    device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "    torch.cuda.device_count()\n",
    "    print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "else :\n",
    "    device=DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344f167",
   "metadata": {},
   "source": [
    "# The inference method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbff75",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def inference(model, inference_cond, Ti_context_length, vocab_size, num_tokens, inference_steps, topn, fname) :\n",
    "    model.eval()\n",
    "    mask = generate_mask(Ti_context_length, Ti_context_length).to(device)\n",
    "\n",
    "    print(f'In inference, the shape of the mask is {mask.shape}')\n",
    "    print(f'In inference, the shape of the inference_cond passed in is  {inference_cond.shape}')\n",
    "\n",
    "    # The \"input data\" is random with a sequence length equal to the context length (and the mask) which is used \n",
    "    # to generate the first step of the output.It is not included in the output.\n",
    "    input_data = torch.randint(0, vocab_size, (1, Ti_context_length, num_tokens)).to(device)  # Smaller context window for inference\n",
    "    #Extend the first conditional vector to cover the \"input\" which is of length Ti_context_length\n",
    "    inference_cond = torch.cat([inference_cond[:, :1, :].repeat(1, Ti_context_length, 1), inference_cond], dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    print(f' In inference, the shape of input_data (context window) is {input_data.shape}')\n",
    "    print(f'In inference, the shape of the inference_cond After extending to cover priming input, is  {inference_cond.shape}')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for i in range(inference_steps):  # \n",
    "        if cond_size == 0:\n",
    "            output = model(input_data, None, mask) # step through \n",
    "        else : \n",
    "            output = model(input_data, inference_cond[:, i:Ti_context_length+i, :], mask) # step through\n",
    "\n",
    "        # This takes the last vector of the sequence (the new predicted token stack) so has size(b,steps,4,1024)\n",
    "        # This it takes the max across the last dimension (scores for each element of the vocabulary (for each of the 4 tokens))\n",
    "        # .max returns a duple of tensors, the first are the max vals (one for each token) and the second are the\n",
    "        #        indices in the range of the vocabulary size. \n",
    "        # THAT IS, the selected \"best\" tokens (one for each codebook) are taken independently\n",
    "        ########################### next_token = output[:, -1, :, :].max(-1)[1]  # Greedy decoding for simplicity\n",
    "        \n",
    "        next_token = sample_top_n(output[:, -1, :, :],topn) # topn=1 would be the same as max in the comment line above    \n",
    "        predictions.append(next_token)\n",
    "        input_data = torch.cat([input_data, next_token.unsqueeze(1)], dim=1)[:, 1:]  # Slide window\n",
    "\n",
    "    t1 = time.time()\n",
    "    inf_time = t1-t0\n",
    "    print(f'inference time for {inference_steps} steps, or {inference_steps/86} seconds of sound is {inf_time}' )\n",
    "\n",
    "    dacseq = torch.cat(predictions, dim=0).unsqueeze(0).transpose(1, 2)\n",
    "    if mask == None:\n",
    "        writeDACFile(fname + '_unmasked', dacseq)\n",
    "    else :\n",
    "        writeDACFile(fname, dacseq)   \n",
    "\n",
    "    print(f'dacseq shape written to file is of shape {dacseq.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the stored model\n",
    "model, _, Ti_context_length, vocab_size, num_codebooks, cond_size = load_model(checkpoint_path,  TransformerClass, DEVICE)\n",
    "\n",
    "print(f'Mode loaded, context_length (Ti_context_length) = {Ti_context_length}')\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e645e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the conditioning sequence from the breakpoint sequence, and plot the trajectories\n",
    "\n",
    "if cond_size == 0 :\n",
    "    inference_cond = None\n",
    "else : \n",
    "\n",
    "    inference_cond=interpolate_vectors(morphs[morphname]['vsequence'], [round(x * inference_steps) for x in morphs[morphname]['vtimes']]) #length must cover staring context window+inf steps\n",
    "\n",
    "\n",
    "    # Make a plot of any changing parmaeters --------------------------------------------------------------\n",
    "    # Extract the 2D array of shape [n, m]\n",
    "    data = inference_cond[0]\n",
    "    # Find components that change over time\n",
    "    changing_indices = [i for i in range(cond_size) if not torch.all(data[:, i] == data[0, i])]\n",
    "\n",
    "    # Plot the changing components\n",
    "    plt.figure(figsize=(10, 3))\n",
    "\n",
    "    for i in changing_indices:\n",
    "        if i != len(classes) :\n",
    "            plt.plot(data[:, i], label=f'{dataset.int2classname[i]} ({i})')\n",
    "        else : \n",
    "            plt.plot(data[:, i], label=f'Parameter ({i})', linestyle='--')\n",
    "\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Component Values')\n",
    "    plt.title(f' {morphname}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    inference_cond=inference_cond.to(device)\n",
    "    print(f'shape of inf_cond is  = {inference_cond.shape}') \n",
    "\n",
    "\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9890f8",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d894ce",
   "metadata": {},
   "source": [
    "# Run the Transformer to generate the .dac file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfname=outdir+\"/\"+ \"dacs\" + \"/\" +  morphname + '_chkpt_' + str(cptnum).zfill(4) +  \"_steps_\"+str(inference_steps).zfill(4) +'.topn_'+ f\"{topn:04d}\"\n",
    "print(f'outfname is {outfname}')\n",
    "inference(model, inference_cond, Ti_context_length, vocab_size, num_codebooks, inference_steps, topn, outfname ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ebcb6",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe3681",
   "metadata": {},
   "source": [
    "# Decode the transformer-generated tokens to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained encodec from Descript\n",
    "# The first time you do this, it can take a while. Go get coffee. After that, it uses a cached version\n",
    "\n",
    "dacmodel_path = dac.utils.download(model_type=\"44khz\") \n",
    "print(f'The DAC decoder is in {dacmodel_path}')\n",
    "with torch.no_grad():\n",
    "    dacmodel = dac.DAC.load(dacmodel_path)\n",
    "\n",
    "    dacmodel.to(device); #wanna see the model? remove the semicolon\n",
    "    dacmodel.eval();  # need to be \"in eval mode\" in order to set the number of quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288a23f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- \n",
    "# --------  Draw the spectrogram \n",
    "# ------------------------------- \n",
    "selected_file=outfname + \".dac\"\n",
    "print(f' selected_file is {selected_file}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    dacfile = dac.DACFile.load(selected_file)\n",
    "    # FIRST - Decompress it back to an AudioSignal\\ from codes to z (1024) to signal   \n",
    "    print(f'dacfile.codes shape is: {dacfile.codes.shape}')\n",
    "    t0=time.time()\n",
    "    asig=dacmodel.decompress(dacfile)\n",
    "    t1=time.time()\n",
    "    \n",
    "    inf_time = t1-t0\n",
    "    print(f'decompress time for {asig.audio_data.shape[2]/44100} seconds of sound is {inf_time}' )\n",
    "    print(f'asig.audio_data.shape[2] is {asig.audio_data.shape[2]}')\n",
    "    \n",
    "    asig.cpu().widget()\n",
    "    asig.audio_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94495345",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = asig.samples.view(-1).numpy()\n",
    "if SAVEWAV :  \n",
    "    sf.write(outfname + \".wav\", adata, 44100)\n",
    "    asig.save_image(outfname + \".jpg\")\n",
    "    print(f'saving .wav and .jpg to outfname {outfname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio display\n",
    "plt.plot(adata)\n",
    "# Audio player\n",
    "ipd.Audio(adata, rate=44100) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
